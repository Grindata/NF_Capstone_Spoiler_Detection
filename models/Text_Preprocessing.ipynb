{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Silver Speech and Golden Silence: Spoiler Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Baseline Model (SGD-Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.utils import resample\n",
    "from langdetect import detect\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable scientific notation for floats\n",
    "pd.options.display.float_format = '{:,}'.format\n",
    "\n",
    "#Enable viewing more (in this case: all) features of a dataset\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datafile\n",
    "train = pd.read_json('/Volumes/My Passport OSX/NF_Capstone_Spoiler_Detection/data/train_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is very large, we only take a sample of about 25,000 and only apply the code on this sample before executing it on all data.\n",
    "Since the data is imbalanced with regard to the target feature, we take a balanced sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample from the data \n",
    "def get_sample(df):\n",
    "    sample = df[['review','spoiler','genre','spoiler_dum', 'sentence_labels', 'review_texts',\n",
    "                      'review_len']].sample(n = 200000, random_state = 42).reset_index(drop = True) \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = get_sample(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the data is imbalanced, we downsample the non-spoilers. This further reduces the sample to about 25,000.\n",
    "def downsample_nonspoilers(df):\n",
    "    df_majority = df[df['spoiler_dum'] == 0] #nonspoilers\n",
    "    df_minority = df[df['spoiler_dum'] == 1] #spoilers\n",
    "    \n",
    "    # Downsample majority labels equal to the number of samples in the minority class\n",
    "    df_majority = df_majority.sample(len(df_minority), random_state = 42)\n",
    "\n",
    "    # Concatenate the majority and minority dataframes\n",
    "    sample = pd.concat([df_majority, df_minority])\n",
    "    \n",
    "    sample.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = downsample_nonspoilers(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.spoiler.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save subsample\n",
    "sample.to_json('data/train_sample.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if all reviews are really written in English by using langdetect.\n",
    "The language code is written in a new column 'lang'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language detection\n",
    "#Let's check if all reviews are really written in English by using langdetect.\n",
    "# The language code is written in a new column 'lang'. \n",
    "# Then, all non-english cases are dropped.\n",
    "def lang_det(df):\n",
    "    df['lang'] = pd.Series()\n",
    "    for i in tqdm(range(len(df))):\n",
    "        df['lang'][i] = detect(df['review_texts'][i])\n",
    "    df.drop(df[df['lang'] != 'en'].index, inplace = True)\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = lang_det(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.lang.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to adapt the reviews before we can feed them to a model:\n",
    "1. Make all words lower case\n",
    "2. Noise Removal\n",
    "  * remove links, email adresses etc\n",
    "  * numbers and special characters\n",
    "  * remove stop words \n",
    "  * lemmatize \n",
    "3. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace short forms/ enlarge contractions\n",
    "#https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "def decontracted(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    \n",
    "    #words\n",
    "    text = re.sub(\"gimme\", \"give me\", text)\n",
    "    text = re.sub(\"cuz\", \"because\", text)\n",
    "    text = re.sub(\"'cause\", \"give me\", text)\n",
    "    text = re.sub(\"finna\", \"fixing to\", text)\n",
    "    text = re.sub(\"cuz\", \"because\", text)\n",
    "    text = re.sub(\"wanna\", \"want to\", text)\n",
    "    text = re.sub(\"gotta\", \"got to\", text)\n",
    "    text = re.sub(\"hafta\", \"have to\", text)\n",
    "    text = re.sub(\"woulda\", \"would have\", text)\n",
    "    text = re.sub(\"coulda\", \"could have\", text)\n",
    "    text = re.sub(\"shoulda\", \"should have\", text)\n",
    "    text = re.sub(\"ma'am\", \"madam\", text)\n",
    "    text = re.sub(\"howdy\", \"how do you\", text)\n",
    "    text = re.sub(\"let's\", \"let us\", text)\n",
    "    text = re.sub(\"y'all\", \"you all\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Noise Removal\n",
    "def preprocessing(text):\n",
    "    #t0 = time.time()\n",
    "    # remove '--' and replace them with whitespace\n",
    "    text = text.replace('-', ' ')\n",
    "    #change to lower case\n",
    "    text = text.lower()\n",
    "    # replace contractions\n",
    "    text = decontracted(text)\n",
    "    #remove urls if there are any\n",
    "    text = re.sub(r'http:\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text)\n",
    "    #remove emails and words containing @\n",
    "    text = re.sub(\"\\S*@\\S*\\s?\",\" \", text)\n",
    "    # remove digits and words containing digits\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[(,.;:@#?!&$)\"*/-]+', ' ', text)\n",
    "    text = re.sub(r\"[']\", '', text)\n",
    "    # replace whitespaces\n",
    "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Spellcheck (includes preprocessing function)\n",
    "from spellchecker import SpellChecker\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "symspell1 = SymSpell()\n",
    "\n",
    "def spellcheck(text):\n",
    "    '''\n",
    "    function takes string as input, preprocesses text and returns a spellchecked text\n",
    "    '''\n",
    "    #preprocess text\n",
    "    text = preprocessing(text)\n",
    "    #hand to spellchecking and return the best result only if corrected phrase is more than just a single letter\n",
    "    # otherwise continue\n",
    "    sp = symspell1.lookup_compound(text, max_edit_distance=1)\n",
    "    if sp[0].term == None:\n",
    "        return text\n",
    "    else:\n",
    "        return sp[0].term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for lemmatization\n",
    "nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatizer(text):\n",
    "    '''\n",
    "    tokenizes string input using spacy \n",
    "    removes english stopwords\n",
    "    '''\n",
    "    doc = nlp_spacy(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New sample feature with preprocessed reviews\n",
    "sample['prep'] = pd.Series(list)\n",
    "\n",
    "for row in tqdm(range(len(sample))):\n",
    "    sample['prep'][row] = []\n",
    "    for sentence in sample.review[row]:\n",
    "        sample['prep'][row].append(spellcheck(sentence[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New sample feature with lemmatized data\n",
    "sample['tokenized'] = pd.Series(list)\n",
    "\n",
    "for row in tqdm(range(len(sample))):\n",
    "    sample['tokenized'][row] = []\n",
    "    for sentence in sample.prep[row]:\n",
    "        sample['tokenized'][row].append(lemmatizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all 1-sentence-reviews for these contain not much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to drop one-sentence-reviews\n",
    "def drop_onesen(df):\n",
    "    df['review_len'][row] = len(df['review'][row])\n",
    "    df.drop(df[df['review_len'] == 1].index, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_onesen(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
