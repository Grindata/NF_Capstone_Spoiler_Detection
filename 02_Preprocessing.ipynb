{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spoiler Alert! Spoiler Detection Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datafile\n",
    "df = pd.read_hdf('data/complete_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, validation and test test with ratios 70% - 20% -10%\n",
    "train, validation, test = np.split(df.sample(frac=1), [int(.7*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/pandas/core/generic.py:2446: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block0_values] [items->Index(['user_id', 'time', 'review', 'book_id', 'review_id', 'genres', 'title',\n",
      "       'description', 'publication_year', 'publication_month',\n",
      "       'publication_day', 'average_rating', 'ratings_count', 'num_pages'],\n",
      "      dtype='object')]\n",
      "\n",
      "  encoding=encoding,\n"
     ]
    }
   ],
   "source": [
    "#Save validation and test sets (train set will be saved after preprocessing) as HDF5\n",
    "test.to_hdf('data/test_data.h5', key = 'test')\n",
    "validation.to_hdf('data/validation_data.h5', key = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, only the train data is manipulated, validation and test sets are only worked on just before model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index\n",
    "train = train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the feature containing the old index\n",
    "train.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 964623 entries, 0 to 964622\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   user_id            964623 non-null  object\n",
      " 1   time               964623 non-null  object\n",
      " 2   review             964623 non-null  object\n",
      " 3   rating             964623 non-null  int64 \n",
      " 4   spoiler            964623 non-null  bool  \n",
      " 5   book_id            964623 non-null  object\n",
      " 6   review_id          964623 non-null  object\n",
      " 7   genres             964623 non-null  object\n",
      " 8   title              964623 non-null  object\n",
      " 9   description        964623 non-null  object\n",
      " 10  publication_year   964623 non-null  object\n",
      " 11  publication_month  964623 non-null  object\n",
      " 12  publication_day    964623 non-null  object\n",
      " 13  average_rating     964623 non-null  object\n",
      " 14  ratings_count      964623 non-null  object\n",
      " 15  num_pages          964623 non-null  object\n",
      "dtypes: bool(1), int64(1), object(14)\n",
      "memory usage: 111.3+ MB\n"
     ]
    }
   ],
   "source": [
    "#Information on data types and missing values\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379862    {'young-adult': 998, 'fantasy, paranormal': 13...\n",
       "875606    {'young-adult': 998, 'fantasy, paranormal': 13...\n",
       "956124    {'young-adult': 998, 'fantasy, paranormal': 13...\n",
       "413489    {'young-adult': 998, 'fantasy, paranormal': 13...\n",
       "811133    {'young-adult': 998, 'fantasy, paranormal': 13...\n",
       "                                ...                        \n",
       "646519                      {'children': 102, 'fiction': 7}\n",
       "905797                      {'children': 102, 'fiction': 7}\n",
       "406255                      {'children': 102, 'fiction': 7}\n",
       "599548                      {'children': 102, 'fiction': 7}\n",
       "696350                      {'children': 102, 'fiction': 7}\n",
       "Name: genres, Length: 964623, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show unique values in the sorted genres column. Since the genres column contains dictionaries, \n",
    "#the data type is temporarily changed to string format. \n",
    "values = train.genres.astype('str').sort_values(ascending = False)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data and data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, some changes are necessary:\n",
    "* Missing values are denoted as '' or '[]', respectively, and need to be changed to np.nan\n",
    "* Datatypes need to be changed for some variables:\n",
    "  * _time_ to date\n",
    "  * _book_id_ to string\n",
    "  * _publication_year_, _publication_month_, _publication_day_, _average_rating_, _ratings_count_, _num_pages_ to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the data type of book_id to string\n",
    "train.book_id = train.book_id.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'time', 'review', 'rating', 'spoiler', 'book_id',\n",
       "       'review_id', 'genres', 'title', 'description', 'publication_year',\n",
       "       'publication_month', 'publication_day', 'average_rating',\n",
       "       'ratings_count', 'num_pages'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the data type of time from object to date in the format (YYYY-MM-DD)\n",
    "from datetime import datetime\n",
    "train.time = pd.to_datetime(train.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change datatypes from object to floats.\n",
    "to_num = ['average_rating', 'ratings_count', 'publication_year', 'publication_month', 'publication_day']\n",
    "for col in to_num:\n",
    "    train[col] = pd.to_numeric(train[col], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a new feature containing the frequency-weighted average of book ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['weighted_avg_rating'] = train.average_rating * train.ratings_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genres column contains more than one genre assignment to the books. Since we only want one genre per book, we create a new column containing the genre most frequently allocated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function fetching the most frequent (= value) genre (= key)\n",
    "import operator\n",
    "\n",
    "def get_genre(dic):\n",
    "    \n",
    "    ''' Return the key of the highest value of dictionary given in.\n",
    "    If the dictionary is empty, return np.nan\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        x = max(dic.items(), key = operator.itemgetter(1))[0]\n",
    "        return x\n",
    "    except:\n",
    "        return np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function defined above to fetch the most frequent genre allocation.\n",
    "#First, write all keys to a list.\n",
    "genre = []\n",
    "for i in range(len(train)):\n",
    "    a = get_genre(train.genres[i])\n",
    "    genre.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the information from the list as a new column to the genre dataframe\n",
    "train['genre'] = pd.Series(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute another column with overall spoiler labels coded as 0 = \"no spoiler\" and 1 = \"spoiler\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spoiler_dum'] = np.where(train['spoiler']== False, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To also have sentence-wise labels and review text without labels, we define and apply the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only the labels 0 and 1 from the review\n",
    "def get_labels(x):\n",
    "    return [label for label, text in x]\n",
    "\n",
    "#Get only the text from the review\n",
    "def get_text(x):\n",
    "    return [text for label, text in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the function to the data\n",
    "train['sentence_labels'] = train.review.apply(lambda x: get_labels(x))\n",
    "train['review_texts'] = train.review.apply(lambda x: get_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.review_texts = train.review_texts.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to delete special and digits characters from the review text and lower the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import special characters, transform them to list and add digits\n",
    "from string import punctuation\n",
    "specials = list(punctuation)\n",
    "specials.extend(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['raw_text'] = pd.Series('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in range(len(train)):\n",
    "    train['raw_text'][i] = re.sub('[^a-zA-Z \" \"]', '', train['review_texts'].copy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally compute two features with the length (word-wise) of each review since one can hypothesize that longer reviews are more likely to contain spoilers than shorter ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['review_len'] = train.review_texts.str.split(' ').map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Denote missing values as np.nan instead of ''.  \n",
    "train.replace('', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to see how many missing values are in every column (as relative frequencies): \n",
    "for col in train.columns:\n",
    "    pct_missing = np.mean(train[col].isna())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publication year, month and day also contain missing values. We drop month and day features as well as the rows with missing values for the publication year.\n",
    "Missing values for num_pages are also dropped.\n",
    "\n",
    "This means a reduction of the train set by 12.3%. Since all features containing NaNs are not of major importance, the reduced dataset is stored separately and will be used only when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the original train set\n",
    "train_red = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns and rows not needed in the copied dataframe\n",
    "train_red.drop(columns = ['publication_month', 'publication_day'], axis = 1, inplace = True)\n",
    "train_red.dropna(subset = ['publication_year', 'num_pages'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any duplicates are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.duplicated()]\n",
    "train_red[train_red.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop_duplicates(inplace = True)\n",
    "train_red.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore boxplots of numeric features for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = pyplot.subplots(ncols=4, figsize=(12, 5), sharey = False)\n",
    "train[['rating', 'publication_year', 'average_rating', 'ratings_count', 'num_pages', 'review_len']].boxplot(return_type='axes', ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, outliers play only a minor role for our project: our main subject is the classification of reviews with regard to spoilers, which might be modulated by other features (we will learn about that in the EDA) but ... Therefore, outliers will not be removed from the dataframe but we will account for them in the feature standardization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safe the reduced dataframe as HDF5\n",
    "train_red.to_hdf('data/train_reduced.h5', key = 'red')\n",
    "\n",
    "#Save the not reduced dataframe as HDF5\n",
    "train_.to_hdf('data/train_data.h5', key = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rescaling of numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are rescaled using the RobustScaler.\n",
    "The centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers. Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale data\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "#Select numerica variables\n",
    "train_num = train.copy().select_dtypes('number')\n",
    "\n",
    "#Robust Scaler\n",
    "mapper = DataFrameMapper([(train_num, RobustScaler())])\n",
    "scaled_features = mapper.fit_transform(train_num.copy(), 4)\n",
    "train_num_scaled = pd.DataFrame(scaled_features, index=train_num.index, columns=train_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rescale reduced data\n",
    "train_red_num = train_red.copy().select_dtypes('number')\n",
    "mapper = DataFrameMapper([(train_red_num, RobustScaler())])\n",
    "scaled_features = mapper.fit_transform(train_red_num.copy(), 4)\n",
    "train_num_red_scaled = pd.DataFrame(scaled_features, index=train_red_num.index, columns=train_red_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate the non-numeric variables\n",
    "train_obj = train.copy().selectdtypes('object', 'datetime')\n",
    "train_red_obj = train_red.copy().selectdtypes('object', 'datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train_scaled = pd.concat([train_num_scaled, train_obj],axis = 1)\n",
    "train_red_scaled = pd.concat([train_num_red_scaled, train_obj],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safe the reduced dataframe as HDF5\n",
    "train_red_scaled.to_hdf('data/train_reduced_scaled.h5', key = 'red')\n",
    "\n",
    "#Save the not reduced dataframe as HDF5\n",
    "train_scaled.to_hdf('data/train_data_scaled.h5', key = 'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
